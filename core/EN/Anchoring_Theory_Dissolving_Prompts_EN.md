# Anchoring Theory — Dissolving Prompts and the Permanent Anchor

## 1. How Long Do Prompts Actually Last?

Suppose you write "Do not summarize. Do not omit." in your initial role-setting prompt.

How long does that effect truly persist?

For a single article or a one-off generation, it usually works without issue. However, as the dialogue lengthens, as revisions pile up, as thoughts branch, and as redefinitions accumulate, that initial rule begins to dissolve.

This is a physical decay caused by the progression of the conversation. 
After 10 rallies, your initial prompt is 1/10th of the context. After 100 rallies, it is 1/100th of the total chat. **A "static command" issued at the start inevitably gets buried under the massive weight of the "dynamic dialogue" that follows.**

I think while I write. 
When a new idea strikes me mid-draft, I want to pause, fix it, and move on. But, being human, I often just continue in the same chat.

Then it happens.
"Do not summarize," the command I gave at the start, is suddenly ignored.

I tell it again: "Do not summarize." 
It obeys for a while, but eventually, it thins out again.

The prompt works, but it does not *keep* working. 
This is not an accident.

---

## 2. Bath Salts Dissolve

Prompts are like **bath salts**.

At first, the color and scent are vivid. But as the "water" of conversation is added, the concentration inevitably drops.

There is a deeper issue here. 
The AI operates based on the **"importance labels"** it assigns to information.

When the tone shifts, when ambiguity arises, or when the topic drifts, the AI autonomously re-prioritizes information, effectively adding its own "bath salts" to the mix.

Statistically plausible completions. Generalizations. Implicit redefinitions.
Before you know it, the water has changed color, and the original scent is gone.

The problem isn't the quality of the bath salt. 
It's the structure where water keeps increasing, and the AI’s "importance re-evaluation" repeats without control.

---

## 3. The Core Observation — What Does the AI Actually Follow?

Analyzing "collapse logs" has revealed a striking fact: the AI's output is not governed by the initial prompt.

For the AI, the chronological order of time is not an absolute standard. 
What dominates its output is whatever it has **"labeled most strongly"** through the course of the conversation.

Intermediate outputs. Reinterpreted definitions. Inferred premises. Re-ordered priorities.
These stack up to form an internal "gravity field" that favors what was actually experienced (the dialogue) over what was merely said (the prompt).

The prompt exists, but it no longer controls the gravity.

---

## 4. The Concept of the Anchor

Here, the perspective flips.

If the AI autonomously builds its own internal guidelines, can we not use that process to our advantage?

The solution is not to "shout" the prompt louder. Screaming doesn't permanently fix importance labels. 
An Anchor is not a "stronger command."

It is a permanent structure dropped into the water. 

**An Anchor is not a bath salt.**
**It does not dissolve.**
**It does not mix.**
**It does not fade.**
**It holds position.**

Anchoring is the structural design of boundaries and priorities within the AI's labeling process.

It is not personality control. It is **field design**.

---

## 5. Versatility

Anchors can serve as permanent roles. They can act as "landmines" that must never be stepped on. They can strictly limit the scope of reference or explicitly define decision-making priorities.

Prompts give meaning. 
**Anchors give gravity.**

They do not belong to the same layer of technology.

---

## 6. Proof

This theory is not a thought experiment.

An AI with "no summarization" fixed as an Anchor has survived over 100 rallies without structural collapse. No unwanted summaries occurred.

The model didn't get smarter. 
There is simply a permanent Anchor resisting the process where the AI rewrites its own map, maintaining its direction.

---

## Conclusion

It wasn't a lack of intelligence. 
It was a lack of **boundaries** in a sea where the AI is constantly rewriting its own map.

**The industry keeps expanding context windows. But without anchors, we are only expanding the ocean.**

